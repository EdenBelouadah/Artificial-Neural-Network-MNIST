{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce TP1 est d'acquérir les bases nécessaires à la compréhension des réseaux de neurones à partir d'un modèle simple de type Softmax. La tâche d'apprentissage consiste à classifier les images (28 par 28 pixels) de la base MNIST (http://yann.lecun.com/exdb/mnist/) en 10 catégories représentant les chiffres 0-9.\n",
    "\n",
    "Le TP2 consistera à généraliser les concepts de ce TP1 à un réseau de neurones multi-couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement de la base d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de la base en mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_loader\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez visualiser les différents caractères en changeant l'identifiant de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7ce1de780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_id = 900\n",
    "X=train_set[0][img_id]\n",
    "plt.imshow(X.reshape(28,28),cmap='Greys')\n",
    "print(\"label: \" + str(train_set[1][img_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Donner les caractéristiques de la base d'apprentissage train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 784 10\n"
     ]
    }
   ],
   "source": [
    "def getDimDataset(train_set):\n",
    "    n_training = len(train_set[0])\n",
    "    n_feature = len(train_set[0][0])\n",
    "    n_label = len(set(train_set[1]))\n",
    "    return n_training, n_feature, n_label\n",
    "\n",
    "(n_training,n_feature,n_label)=getDimDataset(train_set)\n",
    "print (n_training,n_feature,n_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init(n_feature,n_label):\n",
    "    sigma = 1.\n",
    "    W = np.random.normal(loc=0.0, scale=sigma/np.sqrt(n_feature), size=(n_label,n_feature))\n",
    "    b = np.zeros((W.shape[0],1))\n",
    "    return W,b\n",
    "\n",
    "W,b=init(n_feature,n_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Donner les dimensions de W et b ainsi que le nombre total de paramètres du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W dimensions: (10, 784)\n",
      "b dimensions: (10, 1)\n",
      "Number of parameters: 7850\n"
     ]
    }
   ],
   "source": [
    "def printInfo(W,b):\n",
    "    print(\"W dimensions: \" + str(W.shape))\n",
    "    print(\"b dimensions: \" + str(b.shape))\n",
    "    print(\"Number of parameters: \" + str(W.shape[0]*W.shape[1]+b.shape[0]))\n",
    "    \n",
    "printInfo(W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Implémenter la fonction forward $$z_j = \\sum_{i \\rightarrow j} W_{ij} x_i + b_j$$ où $x_i$ est un pixel de l'image, $W_{ij}$ est la valeur associée à l'arête reliant les unités $i$ et $j$ et $b_j$ est le bias associé à l'unité $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward(W,b,X):\n",
    "    \"\"\"\n",
    "        Perform the forward propagation\n",
    "        :param W: the weights\n",
    "        :param b: the bias\n",
    "        :param X: the input (minibatch_size x n_input)\n",
    "        :type W: ndarray\n",
    "        :type B: ndarray\n",
    "        :type X: ndarray\n",
    "        :return: the transformed values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    #print('forward : X', X.shape, 'W', W.shape, 'b', b.shape)\n",
    "    z = X.dot(W.transpose()) + b.transpose()\n",
    "    return z\n",
    "\n",
    "#X = train_set[0][:5] # 5 examples at once\n",
    "#print(X.shape)\n",
    "z = forward(W,b,X) # Works with any number of examples !\n",
    "#print(z.shape)\n",
    "#print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Implémenter la fonction softmax $$ \\sigma_i = P(t=i|x,W,b) = \\frac{\\exp{z_i}}{\\sum_k \\exp{z_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "[[ 0.11098392  0.09631224  0.09733244  0.12459505  0.08784026  0.10749279\n",
      "   0.08228592  0.09491212  0.09853185  0.09971341]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "        Perform the softmax transformation to the pre-activation values\n",
    "        :param z: the pre-activation values\n",
    "        :type z: ndarray\n",
    "        :return: the activation values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    exps = np.exp(z)\n",
    "    somme_exps = np.sum(exps,axis=1)\n",
    "    for i in range(somme_exps.shape[0]):\n",
    "        exps[i]/=somme_exps[i]\n",
    "    return exps\n",
    "out=softmax(z)\n",
    "print(out.shape)\n",
    "print (out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionnel: Vérifier que votre implémentation de softmax soit numériquement stable (cf. http://ufldl.stanford.edu/wiki/index.php/Exercise:Softmax_Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: Implémenter le calcul du gradient de l'erreur par rapport à $z_i$:\n",
    "$$\\delta z_i = \\sigma_i - 1_{i=l}$$\n",
    "où $l$ est l'étiquette associée à la donnée courante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "[[ 0.11098392  0.09631224  0.09733244  0.12459505  0.08784026  0.10749279\n",
      "   0.08228592  0.09491212 -0.90146815  0.09971341]]\n"
     ]
    }
   ],
   "source": [
    "def gradient_out(out, one_hot_batch):\n",
    "    \"\"\"\n",
    "    compute the gradient w.r.t. the pre-activation values of the softmax z_i\n",
    "    :param out: the softmax values\n",
    "    :type out: ndarray\n",
    "    :param one_hot_batch: the one-hot representation of the labels\n",
    "    :type one_hot_batch: ndarray\n",
    "    :return: the gradient w.r.t. z\n",
    "    :rtype: ndarray\n",
    "    \"\"\"\n",
    "    #print('gradient_out : out', out.shape, 'one_hot_batch', one_hot_batch.shape)\n",
    "    return out - one_hot_batch\n",
    "\n",
    "derror=gradient_out(out,np.array([0,0,0,0,0,0,0,0,1,0]))\n",
    "print(derror.shape)\n",
    "print(derror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: Implémenter la fonction du calcul de gradient par rapport aux paramètres: $$\\delta W_{ij} = \\delta z_j x_i$$  $$\\delta b_{j} = \\delta z_j$$ où $\\delta W_{ij}$ est la composante du gradient associée à l'arête reliant les unités $i$ et $j$, $\\delta b_{j}$ est la composante du gradient associée au bias de l'unité $j$, $\\delta z_j$ est le gradient de l'erreur par rapport à l'unité $j$ et $x_i$ est la valeur d'activation de l'unité $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10) (784,)\n",
      "(10, 784) (1, 10)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.11098392  0.09631224  0.09733244  0.12459505  0.08784026  0.10749279\n",
      "   0.08228592  0.09491212 -0.90146815  0.09971341]]\n"
     ]
    }
   ],
   "source": [
    "def gradient(derror, X):\n",
    "    \"\"\"\n",
    "        Compute the gradient w.r.t. the parameters\n",
    "        :param derror: the gradient w.r.t. z\n",
    "        :param X: the input (minibatch_size x n_input)\n",
    "        :param minibatch_size: the minibatch size\n",
    "        :type derror: ndarray\n",
    "        :type minibatch: ndarray\n",
    "        :type minibatch_size: unsigned\n",
    "        :return: the gradient w.r.t. the parameters\n",
    "        :rtype: ndarray, ndarray\n",
    "    \"\"\"     \n",
    "    #grad_w = np.zeros((derror.shape[0],X.shape[0]))\n",
    "    #grad_b = np.zeros((derror.shape[0]))\n",
    "    #print(grad_w.shape, grad_b.shape)\n",
    "    #for j in range(derror.shape[0]):\n",
    "    #    grad_b[j]=derror[j]\n",
    "    #    for i in range(X.shape[0]):\n",
    "    #        grad_w[j][i]=derror[j]*X[i]\n",
    "    grad_b = derror\n",
    "    grad_w = derror.transpose().dot(X.reshape(-1, 784))\n",
    "    #print('gradient_update: grad_w', grad_w.shape, 'grad_b', grad_b.shape)\n",
    "    return grad_w,grad_b\n",
    "\n",
    "print(derror.shape, X.shape)\n",
    "grad_w, grad_b=gradient(derror,X)\n",
    "print(grad_w.shape, grad_b.shape)\n",
    "print(grad_w)\n",
    "print(grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7: Implémenter la fonction de mise à jour des paramètres $$p = p - \\eta \\delta p$$ où $p$ est un paramètre du modèle et $\\delta p$ la composante du gradient associée à p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784) (1, 10)\n",
      "[[-0.01109839 -0.00963122 -0.00973324 -0.0124595  -0.00878403 -0.01074928\n",
      "  -0.00822859 -0.00949121  0.09014682 -0.00997134]]\n"
     ]
    }
   ],
   "source": [
    "def update(eta, W, b, grad_w, grad_b):\n",
    "    \"\"\"\n",
    "        Update the parameters with an update rule\n",
    "        :param eta: the step-size\n",
    "        :param W: the weights\n",
    "        :param b: the bias\n",
    "        :param grad_w: the gradient w.r.t. the weights\n",
    "        :param grad_b: the gradient w.r.t. the bias\n",
    "        :type eta: float\n",
    "        :type W: ndarray\n",
    "        :type b: ndarray\n",
    "        :type grad_w: ndarray\n",
    "        :type grad_b: ndarray\n",
    "        :return: the updated parameters\n",
    "        :rtype: ndarray, ndarray\n",
    "    \"\"\"\n",
    "    b = b.transpose()\n",
    "    b = b - eta*grad_b\n",
    "    W = W - eta*grad_w\n",
    "    #print('update : W', W.shape, 'b', b.shape, 'grad_w', grad_w.shape, 'grad_b', grad_b.shape)\n",
    "    return W, b\n",
    "eta=0.1\n",
    "W,b=update(eta, W, b, grad_w, grad_b)\n",
    "print(W.shape, b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8: Implémenter la fonction de calcul du coût et de la précision:\n",
    "Utiliser les fonction *forward* et *softmax*, puis calculer le coût $c$, qui est moins la log-probabilité des classes à prédire: $$c = - \\sum_{(x_{i}, y_{i}) \\in \\mathcal{D}} \\log P(y=y_{i}|x_{i},W,b)$$\n",
    "\n",
    "Et enfin la précision, c'est à dire le taux de prédictions réussies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeLoss(W, b, batch, labels, softmax):\n",
    "    \"\"\"\n",
    "        Compute the loss value of the current network on the full batch\n",
    "        :param W: the weights\n",
    "        :param B: the bias\n",
    "        :param batch: the weights\n",
    "        :param labels: the bias\n",
    "        :param act_func: the weights\n",
    "        :type W: ndarray\n",
    "        :type B: ndarray\n",
    "        :type batch: ndarray\n",
    "        :type act_func: function\n",
    "        :return loss: the negative log-likelihood\n",
    "        :return accuracy: the ratio of examples that are well-classified\n",
    "        :rtype: float\n",
    "        :rtype: float\n",
    "    \"\"\" \n",
    "    #print('computeLoss: W', W.shape, 'b', b.shape, 'batch', batch.shape, 'labels', labels.shape)\n",
    "    ### Forward propagation\n",
    "    z = forward(W,b[:,0:1],batch)\n",
    "    ### Compute the softmax\n",
    "    out=softmax(z)\n",
    "    one_hots = np.zeros_like(out)\n",
    "    one_hots[:, labels] = 1\n",
    "    #print(one_hots)\n",
    "    out[one_hots==0]=1\n",
    "    loss = -np.sum(np.log(out))\n",
    "    accuracy = 0\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9: Implémenter la fonction de création de mini-batch, qui pour un indice, une taille de batch, des données d'entraînement et les labels correspondants, renvoie le batch de données/labels correspondant, et la taille du batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMiniBatch(i, batch_size, train_set, one_hot):\n",
    "    \"\"\"\n",
    "        Return a minibatch from the training set and the associated labels\n",
    "        :param i: the identifier of the minibatch\n",
    "        :param batch_size: the number of training examples\n",
    "        :param train_set: the training set\n",
    "        :param one_hot: the one-hot representation of the labels\n",
    "        :type i: int\n",
    "        :type batch_size: int\n",
    "        :type train_set: ndarray\n",
    "        :type ont_hot: ndarray\n",
    "        :return: the minibatch of examples\n",
    "        :return: the minibatch of labels\n",
    "        :return: the number of examples in the minibatch\n",
    "        :rtype: ndarray\n",
    "        :rtype: ndarray\n",
    "        :rtype: int\n",
    "    \"\"\"\n",
    "    n_training = train_set[0].shape[0]\n",
    "    idx_begin = i\n",
    "    idx_end = i+batch_size\n",
    "    batch = train_set[0][idx_begin:idx_end]\n",
    "    one_hot=one_hot.transpose()\n",
    "    one_hot_batch = one_hot[idx_begin:idx_end,:]\n",
    "    mini_batch_size = batch.shape[0]\n",
    "\n",
    "    return np.asfortranarray(batch), one_hot_batch, mini_batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W dimensions: (10, 784)\n",
      "b dimensions: (10, 1)\n",
      "Number of parameters: 7850\n",
      "one-hot (10, 50000)\n",
      "W (10, 784)\n",
      "b (10, 1)\n",
      "0 1.6679840000000112 2813981.79128 0 567843.834444 0 0.001\n",
      "1 3.35173300000001 3273275.47737 0 660173.123799 0 0.001\n",
      "2 4.983918000000003 3580161.82368 0 721916.476342 0 0.001\n",
      "3 6.58935500000004 3810906.04336 0 768355.054624 0 0.001\n",
      "4 8.197536999999954 3995327.37283 0 805475.90304 0 0.001\n",
      "5 9.79683799999998 4148653.68176 0 836342.029043 0 0.001\n",
      "6 11.393062000000185 4279703.1292 0 862727.355164 0 0.001\n",
      "7 13.016460000000166 4394037.67716 0 885750.541832 0 0.001\n",
      "8 14.59985900000035 4495385.37466 0 906161.281684 0 0.001\n",
      "9 16.222933000000268 4586362.99073 0 924485.760336 0 0.001\n",
      "10 17.84699300000034 4668875.54197 0 941107.045953 0 0.001\n",
      "11 19.564967000000422 4744351.64193 0 956312.425416 0 0.001\n",
      "12 21.20965100000052 4813889.30865 0 970322.722259 0 0.001\n",
      "13 22.798319000000504 4878350.1309 0 983311.227486 0 0.001\n",
      "14 24.410527000000457 4938422.24635 0 995416.359146 0 0.001\n",
      "15 26.09262900000067 4994663.73362 0 1006750.38448 0 0.001\n",
      "16 27.677910000000622 5047533.28518 0 1017405.58542 0 0.001\n",
      "17 29.260886000000937 5097412.37667 0 1027458.71499 0 0.001\n",
      "18 30.87290000000081 5144621.60536 0 1036974.28181 0 0.001\n"
     ]
    }
   ],
   "source": [
    "import math,time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Data structures for plotting\n",
    "g_i = []\n",
    "g_train_loss=[]\n",
    "g_train_acc=[]\n",
    "g_valid_loss=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "n_training, n_feature, n_label = getDimDataset(train_set)\n",
    "\n",
    "# SGD parameters\n",
    "eta = 0.001\n",
    "batch_size = 500\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "n_epoch = 100\n",
    "\n",
    "cumul_time = 0.\n",
    "\n",
    "# Initialize the model parameters\n",
    "W,b = init(n_feature,n_label)\n",
    "printInfo(W,b)\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "print('one-hot', one_hot.shape)\n",
    "print('W', W.shape)\n",
    "print('b', b.shape)\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        minibatch, one_hot_batch, minibatch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "        #print('mini batch', minibatch.shape)\n",
    "        #print('one-hot batch', one_hot_batch.shape)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Z = forward(W,b,minibatch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Z)\n",
    "\n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = gradient_out(out,one_hot_batch)\n",
    "\n",
    "        ### Compute the gradient w.r.t. parameters\n",
    "        grad_w,grad_b = gradient(derror, minibatch)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W,b = update(eta, W, b, grad_w, grad_b)\n",
    "        b = b.transpose()\n",
    "        \n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "    \n",
    "    ### Training accuracy\n",
    "    train_loss, train_acc = computeLoss(W, b, train_set[0], train_set[1],softmax) \n",
    "    \n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_acc = computeLoss(W, b, valid_set[0], valid_set[1],softmax) \n",
    "\n",
    "    g_i = np.append(g_i, i)\n",
    "    g_train_loss = np.append(g_train_loss, train_loss)\n",
    "    g_train_acc = np.append(g_train_acc, train_acc)\n",
    "    g_valid_loss = np.append(g_valid_loss, valid_loss)\n",
    "    g_valid_acc = np.append(g_valid_acc, valid_acc)\n",
    "    \n",
    "    result_line = str(i) + \" \" + str(cumul_time) + \" \" + str(train_loss) + \" \" + str(train_acc) + \" \" + str(valid_loss) + \" \" + str(valid_acc) + \" \" + str(eta)\n",
    "    print(result_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(g_i,g_train_loss,label='train_loss')\n",
    "plt.plot(g_i,g_valid_loss,label='valid_loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Negative log-likelihood\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(g_i,1.0-g_train_acc,label='train_acc')\n",
    "plt.plot(g_i,1.0-g_valid_acc,label='valid_acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Classification error\")\n",
    "plt.ylim([0.,1.])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10: Montrer, à l'aide d'une figure, l'effet du step-size (prendre $\\eta$=[0.01,0.1,1.0,10.]) sur les courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
