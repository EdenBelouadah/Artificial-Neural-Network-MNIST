{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce TP1 est d'acquérir les bases nécessaires à la compréhension des réseaux de neurones à partir d'un modèle simple de type Softmax. La tâche d'apprentissage consiste à classifier les images (28 par 28 pixels) de la base MNIST (http://yann.lecun.com/exdb/mnist/) en 10 catégories représentant les chiffres 0-9.\n",
    "\n",
    "Le TP2 consistera à généraliser les concepts de ce TP1 à un réseau de neurones multi-couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement de la base d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de la base en mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_loader\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez visualiser les différents caractères en changeant l'identifiant de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngaAIPxAU4QeCIvxAUP8POQ52fDnQijQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8363550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_id = 900\n",
    "X=train_set[0][img_id]\n",
    "plt.imshow(X.reshape(28,28),cmap='Greys')\n",
    "print(\"label: \" + str(train_set[1][img_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Donner les caractéristiques de la base d'apprentissage train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784, 10)\n"
     ]
    }
   ],
   "source": [
    "def getDimDataset(train_set):\n",
    "    n_training = len(train_set[0])\n",
    "    n_feature = len(train_set[0][0])\n",
    "    n_label = len(set(train_set[1]))\n",
    "    return n_training, n_feature, n_label\n",
    "\n",
    "(n_training,n_feature,n_label)=getDimDataset(train_set)\n",
    "print (n_training,n_feature,n_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def init(n_feature,n_label):\n",
    "    sigma = 1.\n",
    "    W = np.random.normal(loc=0.0, scale=sigma/np.sqrt(n_feature), size=(n_label,n_feature))\n",
    "    b = np.zeros((W.shape[0],1))\n",
    "    return W,b\n",
    "\n",
    "W,b=init(n_feature,n_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Donner les dimensions de W et b ainsi que le nombre total de paramètres du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W dimensions: (10L, 784L)\n",
      "b dimensions: (10L, 1L)\n",
      "Number of parameters: 7850\n"
     ]
    }
   ],
   "source": [
    "def printInfo(W,b):\n",
    "    print(\"W dimensions: \" + str(W.shape))\n",
    "    print(\"b dimensions: \" + str(b.shape))\n",
    "    print(\"Number of parameters: \" + str(W.shape[0]*W.shape[1]+b.shape[0]))\n",
    "    \n",
    "printInfo(W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Implémenter la fonction forward $$z_j = \\sum_{i \\rightarrow j} W_{ij} x_i + b_j$$ où $x_i$ est un pixel de l'image, $W_{ij}$ est la valeur associée à l'arête reliant les unités $i$ et $j$ et $b_j$ est le bias associé à l'unité $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19085243]\n",
      " [-0.28488729]\n",
      " [-0.03912406]\n",
      " [-0.45336725]\n",
      " [-0.56136663]\n",
      " [-0.36939721]\n",
      " [-0.38351951]\n",
      " [-0.11314998]\n",
      " [ 0.34946254]\n",
      " [ 0.21535563]]\n"
     ]
    }
   ],
   "source": [
    "def forward(W,b,X):\n",
    "    \"\"\"\n",
    "        Perform the forward propagation\n",
    "        :param W: the weights\n",
    "        :param b: the bias\n",
    "        :param X: the input (minibatch_size x n_input)\n",
    "        :type W: ndarray\n",
    "        :type B: ndarray\n",
    "        :type X: ndarray\n",
    "        :return: the transformed values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    #z_j = np.zeros(len(b))\n",
    "    #for j in range(0,len(z_j)):\n",
    "    #    somme=0\n",
    "    #    for i in range(0,X.shape[0]):\n",
    "    #        somme+=W[j][i]*X[i]\n",
    "    #    z_j[j]=somme+b[j]\n",
    "    \n",
    "    z = np.zeros((b.shape[0],1))\n",
    "    for j in range(z.shape[0]):\n",
    "        z[j]=W[j].dot(X)+b[j]\n",
    "        \n",
    "    #z=W.dot(X)+b\n",
    "    \n",
    "    return z\n",
    "\n",
    "z=forward(W,b,X)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Implémenter la fonction softmax $$ \\sigma_i = P(t=i|x,W,b) = \\frac{\\exp{z_i}}{\\sum_k \\exp{z_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target=8\n",
      "[[ 0.13367126]\n",
      " [ 0.08306669]\n",
      " [ 0.1062088 ]\n",
      " [ 0.07018705]\n",
      " [ 0.06300187]\n",
      " [ 0.07633517]\n",
      " [ 0.07526472]\n",
      " [ 0.09863055]\n",
      " [ 0.1566468 ]\n",
      " [ 0.13698709]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "        Perform the softmax transformation to the pre-activation values\n",
    "        :param z: the pre-activation values\n",
    "        :type z: ndarray\n",
    "        :return: the activation values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    alpha=4\n",
    "    out= np.zeros((z.shape[0],1))\n",
    "    somme=0\n",
    "    for i in range(len(z)):\n",
    "        out[i]=np.exp(z[i])\n",
    "        somme+=out[i]\n",
    "    out=out/somme\n",
    "    \n",
    "    target= out.argmax()\n",
    "    print 'target='+str(target)\n",
    "    return out\n",
    "out=softmax(z)\n",
    "print (out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionnel: Vérifier que votre implémentation de softmax soit numériquement stable (cf. http://ufldl.stanford.edu/wiki/index.php/Exercise:Softmax_Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ nan,   0.,   0.]), 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karim\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Karim\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Example for testing the numerical stability of softmax\n",
    "# It should return [1., 0. ,0.], not [nan, 0., 0.]\n",
    "z_test = [1000000.0,1.0,100.0]\n",
    "print(softmax(z_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: Implémenter le calcul du gradient de l'erreur par rapport à $z_i$:\n",
    "$$\\delta z_i = \\sigma_i - 1_{i=l}$$\n",
    "où $l$ est l'étiquette associée à la donnée courante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13367126]\n",
      " [ 0.08306669]\n",
      " [ 0.1062088 ]\n",
      " [ 0.07018705]\n",
      " [ 0.06300187]\n",
      " [ 0.07633517]\n",
      " [ 0.07526472]\n",
      " [ 0.09863055]\n",
      " [-0.8433532 ]\n",
      " [ 0.13698709]]\n"
     ]
    }
   ],
   "source": [
    "def gradient_out(out, one_hot_batch):\n",
    "    \"\"\"\n",
    "    compute the gradient w.r.t. the pre-activation values of the softmax z_i\n",
    "    :param out: the softmax values\n",
    "    :type out: ndarray\n",
    "    :param one_hot_batch: the one-hot representation of the labels\n",
    "    :type one_hot_batch: ndarray\n",
    "    :return: the gradient w.r.t. z\n",
    "    :rtype: ndarray\n",
    "    \"\"\"\n",
    "    derror=np.zeros((out.shape[0],1))\n",
    "    for j in range(out.shape[0]):   \n",
    "        derror[j]=out[j]-(one_hot_batch[j]==1)    \n",
    "    return derror\n",
    "derror=gradient_out(out,np.array([0,0,0,0,0,0,0,0,1,0]))\n",
    "print derror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: Implémenter la fonction du calcul de gradient par rapport aux paramètres: $$\\delta W_{ij} = \\delta z_j x_i$$  $$\\delta b_{j} = \\delta z_j$$ où $\\delta W_{ij}$ est la composante du gradient associée à l'arête reliant les unités $i$ et $j$, $\\delta b_{j}$ est la composante du gradient associée au bias de l'unité $j$, $\\delta z_j$ est le gradient de l'erreur par rapport à l'unité $j$ et $x_i$ est la valeur d'activation de l'unité $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10L, 784L) (10L,)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [-0. -0. -0. ..., -0. -0. -0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]] [ 0.13367126  0.08306669  0.1062088   0.07018705  0.06300187  0.07633517\n",
      "  0.07526472  0.09863055 -0.8433532   0.13698709]\n"
     ]
    }
   ],
   "source": [
    "def gradient(derror, X):\n",
    "    \"\"\"\n",
    "        Compute the gradient w.r.t. the parameters\n",
    "        :param derror: the gradient w.r.t. z\n",
    "        :param X: the input (minibatch_size x n_input)\n",
    "        :param minibatch_size: the minibatch size\n",
    "        :type derror: ndarray\n",
    "        :type minibatch: ndarray\n",
    "        :type minibatch_size: unsigned\n",
    "        :return: the gradient w.r.t. the parameters\n",
    "        :rtype: ndarray, ndarray\n",
    "    \"\"\"     \n",
    "    grad_w = np.zeros((derror.shape[0],X.shape[0]))\n",
    "    grad_b = np.zeros((derror.shape[0]))\n",
    "    print grad_w.shape, grad_b.shape\n",
    "    for j in range(derror.shape[0]):\n",
    "        grad_b[j]=derror[j]\n",
    "        for i in range(X.shape[0]):\n",
    "            grad_w[j][i]=derror[j]*X[i]\n",
    "    return grad_w,grad_b\n",
    "grad_w, grad_b=gradient(derror,X)\n",
    "print grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7: Implémenter la fonction de mise à jour des paramètres $$p = p - \\eta \\delta p$$ où $p$ est un paramètre du modèle et $\\delta p$ la composante du gradient associée à p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.33417814]\n",
      " [-0.19936005]\n",
      " [-0.25490112]\n",
      " [-0.16844892]\n",
      " [-0.15120449]\n",
      " [-0.18320442]\n",
      " [-0.18063533]\n",
      " [-0.23671332]\n",
      " [ 2.02404768]\n",
      " [-0.32876901]]\n"
     ]
    }
   ],
   "source": [
    "def update(eta, W, b, grad_w, grad_b):\n",
    "    \"\"\"\n",
    "        Update the parameters with an update rule\n",
    "        :param eta: the step-size\n",
    "        :param W: the weights\n",
    "        :param b: the bias\n",
    "        :param grad_w: the gradient w.r.t. the weights\n",
    "        :param grad_b: the gradient w.r.t. the bias\n",
    "        :type eta: float\n",
    "        :type W: ndarray\n",
    "        :type b: ndarray\n",
    "        :type grad_w: ndarray\n",
    "        :type grad_b: ndarray\n",
    "        :return: the updated parameters\n",
    "        :rtype: ndarray, ndarray\n",
    "    \"\"\"   \n",
    "    b=b-eta*derror\n",
    "    for i in range(len(b)):\n",
    "        for j in range(W.shape[1]):\n",
    "            W[i][j]=W[i][j]-eta*grad_w[i][j] \n",
    "    return W, b\n",
    "eta=0.1\n",
    "W,b=update(eta, W, b, grad_w, grad_b)\n",
    "print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8: Implémenter la fonction de calcul du coût et de la précision:\n",
    "Utiliser les fonction *forward* et *softmax*, puis calculer le coût $c$, qui est moins la log-probabilité des classes à prédire: $$c = - \\sum_{(x_{i}, y_{i}) \\in \\mathcal{D}} \\log P(y=y_{i}|x_{i},W,b)$$\n",
    "\n",
    "Et enfin la précision, c'est à dire le taux de prédictions réussies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeLoss(W, b, batch, labels, softmax):\n",
    "    \"\"\"\n",
    "        Compute the loss value of the current network on the full batch\n",
    "        :param W: the weights\n",
    "        :param B: the bias\n",
    "        :param batch: the weights\n",
    "        :param labels: the bias\n",
    "        :param act_func: the weights\n",
    "        :type W: ndarray\n",
    "        :type B: ndarray\n",
    "        :type batch: ndarray\n",
    "        :type act_func: function\n",
    "        :return loss: the negative log-likelihood\n",
    "        :return accuracy: the ratio of examples that are well-classified\n",
    "        :rtype: float\n",
    "        :rtype: float\n",
    "    \"\"\" \n",
    "    ### Forward propagation\n",
    "    ### Compute the softmax\n",
    "    loss = 0\n",
    "    accuracy = 0         \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9: Implémenter la fonction de création de mini-batch, qui pour un indice, une taille de batch, des données d'entraînement et les labels correspondants, renvoie le batch de données/labels correspondant, et la taille du batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMiniBatch(i, batch_size, train_set, one_hot):\n",
    "    \"\"\"\n",
    "        Return a minibatch from the training set and the associated labels\n",
    "        :param i: the identifier of the minibatch\n",
    "        :param batch_size: the number of training examples\n",
    "        :param train_set: the training set\n",
    "        :param one_hot: the one-hot representation of the labels\n",
    "        :type i: int\n",
    "        :type batch_size: int\n",
    "        :type train_set: ndarray\n",
    "        :type ont_hot: ndarray\n",
    "        :return: the minibatch of examples\n",
    "        :return: the minibatch of labels\n",
    "        :return: the number of examples in the minibatch\n",
    "        :rtype: ndarray\n",
    "        :rtype: ndarray\n",
    "        :rtype: int\n",
    "    \"\"\"\n",
    "    n_training = train_set[0].shape[0]\n",
    "    idx_begin = 0\n",
    "    idx_end = 0\n",
    "    mini_batch_size = 0\n",
    "\n",
    "    batch = 0\n",
    "    one_hot_batch = 0\n",
    "\n",
    "    return np.asfortranarray(batch), one_hot_batch, mini_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math,time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Data structures for plotting\n",
    "g_i = []\n",
    "g_train_loss=[]\n",
    "g_train_acc=[]\n",
    "g_valid_loss=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "n_training, n_feature, n_label = getDimDataset(train_set)\n",
    "\n",
    "# SGD parameters\n",
    "eta = 0.001\n",
    "batch_size = 500\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "n_epoch = 100\n",
    "\n",
    "cumul_time = 0.\n",
    "\n",
    "# Initialize the model parameters\n",
    "W,b = init(n_feature,n_label)\n",
    "printInfo(W,b)\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        minibatch, one_hot_batch, minibatch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Z = forward(W,b,minibatch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Z)\n",
    "\n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = gradient_out(out,one_hot_batch)\n",
    "\n",
    "        ### Compute the gradient w.r.t. parameters\n",
    "        grad_w,grad_b = gradient(derror, minibatch)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W,b = update(eta, W, b, grad_w, grad_b)\n",
    "        \n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "    \n",
    "    ### Training accuracy\n",
    "    train_loss, train_acc = computeLoss(W, b, train_set[0], train_set[1],softmax) \n",
    "    \n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_acc = computeLoss(W, b, valid_set[0], valid_set[1],softmax) \n",
    "\n",
    "    g_i = np.append(g_i, i)\n",
    "    g_train_loss = np.append(g_train_loss, train_loss)\n",
    "    g_train_acc = np.append(g_train_acc, train_acc)\n",
    "    g_valid_loss = np.append(g_valid_loss, valid_loss)\n",
    "    g_valid_acc = np.append(g_valid_acc, valid_acc)\n",
    "    \n",
    "    result_line = str(i) + \" \" + str(cumul_time) + \" \" + str(train_loss) + \" \" + str(train_acc) + \" \" + str(valid_loss) + \" \" + str(valid_acc) + \" \" + str(eta)\n",
    "    print(result_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(g_i,g_train_loss,label='train_loss')\n",
    "plt.plot(g_i,g_valid_loss,label='valid_loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Negative log-likelihood\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(g_i,1.0-g_train_acc,label='train_acc')\n",
    "plt.plot(g_i,1.0-g_valid_acc,label='valid_acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Classification error\")\n",
    "plt.ylim([0.,1.])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10: Montrer, à l'aide d'une figure, l'effet du step-size (prendre $\\eta$=[0.01,0.1,1.0,10.]) sur les courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
