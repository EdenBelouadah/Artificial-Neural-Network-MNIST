{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce TP1 est d'acquérir les bases nécessaires à la compréhension des réseaux de neurones à partir d'un modèle simple de type Softmax. La tâche d'apprentissage consiste à classifier les images (28 par 28 pixels) de la base MNIST (http://yann.lecun.com/exdb/mnist/) en 10 catégories représentant les chiffres 0-9.\n",
    "\n",
    "Le TP2 consistera à généraliser les concepts de ce TP1 à un réseau de neurones multi-couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement de la base d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de la base en mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_loader\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez visualiser les différents caractères en changeant l'identifiant de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIKj/Apwk9X1GSlIbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc20a5db710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_id = 900\n",
    "X=train_set[0][img_id]\n",
    "plt.imshow(X.reshape(28,28),cmap='Greys')\n",
    "print(\"label: \" + str(train_set[1][img_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Donner les caractéristiques de la base d'apprentissage train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 784 10\n"
     ]
    }
   ],
   "source": [
    "def getDimDataset(train_set):\n",
    "    n_training = len(train_set[0])\n",
    "    n_feature = len(train_set[0][0])\n",
    "    n_label = len(set(train_set[1]))\n",
    "    return n_training, n_feature, n_label\n",
    "\n",
    "(n_training,n_feature,n_label)=getDimDataset(train_set)\n",
    "print (n_training,n_feature,n_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(n_feature,n_label):\n",
    "    sigma = 1.\n",
    "    W = np.random.normal(loc=0.0, scale=sigma/np.sqrt(n_feature), size=(n_label,n_feature))\n",
    "    b = np.zeros((W.shape[0],1))\n",
    "    return W,b\n",
    "\n",
    "W,b=init(n_feature,n_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Donner les dimensions de W et b ainsi que le nombre total de paramètres du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W dimensions: (10, 784)\n",
      "b dimensions: (10, 1)\n",
      "Number of parameters: 7850\n"
     ]
    }
   ],
   "source": [
    "def printInfo(W,b):\n",
    "    print(\"W dimensions: \" + str(W.shape))\n",
    "    print(\"b dimensions: \" + str(b.shape))\n",
    "    print(\"Number of parameters: \" + str(W.shape[0]*W.shape[1]+b.shape[0]))\n",
    "    \n",
    "printInfo(W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Implémenter la fonction forward $$z_j = \\sum_{i \\rightarrow j} W_{ij} x_i + b_j$$ où $x_i$ est un pixel de l'image, $W_{ij}$ est la valeur associée à l'arête reliant les unités $i$ et $j$ et $b_j$ est le bias associé à l'unité $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(1, 10)\n",
      "[[-0.35864422  0.32823058 -0.03009426 -0.36506513 -0.14502875  0.1277127\n",
      "   0.37975091  0.21329379 -0.16297015 -0.16475288]]\n"
     ]
    }
   ],
   "source": [
    "def forward(W,b,X):\n",
    "    \"\"\"\n",
    "        Perform the forward propagation\n",
    "        :param W: the weights\n",
    "        :param b: the bias\n",
    "        :param X: the input (minibatch_size x n_input)\n",
    "        :type W: ndarray\n",
    "        :type B: ndarray\n",
    "        :type X: ndarray\n",
    "        :return: the transformed values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    z = X.dot(W.transpose()) + b.transpose()\n",
    "    \n",
    "    return z\n",
    "\n",
    "# X = train_set[0][:5] # 5 examples at once\n",
    "print(X.shape)\n",
    "z = forward(W,b,X) # Works with any number of examples !\n",
    "print(z.shape)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Implémenter la fonction softmax $$ \\sigma_i = P(t=i|x,W,b) = \\frac{\\exp{z_i}}{\\sum_k \\exp{z_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "[[ 0.06882814  0.13679556  0.09559903  0.06838762  0.08521932  0.11194074\n",
      "   0.14402802  0.12194264  0.083704    0.08355491]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "        Perform the softmax transformation to the pre-activation values\n",
    "        :param z: the pre-activation values\n",
    "        :type z: ndarray\n",
    "        :return: the activation values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    exps = np.exp(z)\n",
    "    somme_exps = np.sum(exps)\n",
    "    return exps / somme_exps\n",
    "out=softmax(z)\n",
    "print(out.shape)\n",
    "print (out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionnel: Vérifier que votre implémentation de softmax soit numériquement stable (cf. http://ufldl.stanford.edu/wiki/index.php/Exercise:Softmax_Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ nan   0.   0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel/__main__.py:9: RuntimeWarning: overflow encountered in exp\n",
      "/usr/lib/python3.6/site-packages/ipykernel/__main__.py:11: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# Example for testing the numerical stability of softmax\n",
    "# It should return [1., 0. ,0.], not [nan, 0., 0.]\n",
    "z_test = [1000000.0,1.0,100.0]\n",
    "print(softmax(z_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: Implémenter le calcul du gradient de l'erreur par rapport à $z_i$:\n",
    "$$\\delta z_i = \\sigma_i - 1_{i=l}$$\n",
    "où $l$ est l'étiquette associée à la donnée courante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "[[ 0.06882814  0.13679556  0.09559903  0.06838762  0.08521932  0.11194074\n",
      "   0.14402802  0.12194264 -0.916296    0.08355491]]\n"
     ]
    }
   ],
   "source": [
    "def gradient_out(out, one_hot_batch):\n",
    "    \"\"\"\n",
    "    compute the gradient w.r.t. the pre-activation values of the softmax z_i\n",
    "    :param out: the softmax values\n",
    "    :type out: ndarray\n",
    "    :param one_hot_batch: the one-hot representation of the labels\n",
    "    :type one_hot_batch: ndarray\n",
    "    :return: the gradient w.r.t. z\n",
    "    :rtype: ndarray\n",
    "    \"\"\"   \n",
    "    return out - one_hot_batch\n",
    "\n",
    "derror=gradient_out(out,np.array([0,0,0,0,0,0,0,0,1,0]))\n",
    "print(derror.shape)\n",
    "print(derror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: Implémenter la fonction du calcul de gradient par rapport aux paramètres: $$\\delta W_{ij} = \\delta z_j x_i$$  $$\\delta b_{j} = \\delta z_j$$ où $\\delta W_{ij}$ est la composante du gradient associée à l'arête reliant les unités $i$ et $j$, $\\delta b_{j}$ est la composante du gradient associée au bias de l'unité $j$, $\\delta z_j$ est le gradient de l'erreur par rapport à l'unité $j$ et $x_i$ est la valeur d'activation de l'unité $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10) (784,)\n",
      "(10, 784) (1, 10)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.06882814  0.13679556  0.09559903  0.06838762  0.08521932  0.11194074\n",
      "   0.14402802  0.12194264 -0.916296    0.08355491]]\n"
     ]
    }
   ],
   "source": [
    "def gradient(derror, X):\n",
    "    \"\"\"\n",
    "        Compute the gradient w.r.t. the parameters\n",
    "        :param derror: the gradient w.r.t. z\n",
    "        :param X: the input (minibatch_size x n_input)\n",
    "        :param minibatch_size: the minibatch size\n",
    "        :type derror: ndarray\n",
    "        :type minibatch: ndarray\n",
    "        :type minibatch_size: unsigned\n",
    "        :return: the gradient w.r.t. the parameters\n",
    "        :rtype: ndarray, ndarray\n",
    "    \"\"\"     \n",
    "    #grad_w = np.zeros((derror.shape[0],X.shape[0]))\n",
    "    #grad_b = np.zeros((derror.shape[0]))\n",
    "    #print(grad_w.shape, grad_b.shape)\n",
    "    #for j in range(derror.shape[0]):\n",
    "    #    grad_b[j]=derror[j]\n",
    "    #    for i in range(X.shape[0]):\n",
    "    #        grad_w[j][i]=derror[j]*X[i]\n",
    "    grad_b = derror\n",
    "    grad_w = derror.transpose().dot(X.reshape(-1, 784))\n",
    "    return grad_w,grad_b\n",
    "\n",
    "print(derror.shape, X.shape)\n",
    "grad_w, grad_b=gradient(derror,X)\n",
    "print(grad_w.shape, grad_b.shape)\n",
    "print(grad_w)\n",
    "print(grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7: Implémenter la fonction de mise à jour des paramètres $$p = p - \\eta \\delta p$$ où $p$ est un paramètre du modèle et $\\delta p$ la composante du gradient associée à p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784) (1, 10)\n",
      "[[-0.00688281 -0.01367956 -0.0095599  -0.00683876 -0.00852193 -0.01119407\n",
      "  -0.0144028  -0.01219426  0.0916296  -0.00835549]]\n"
     ]
    }
   ],
   "source": [
    "def update(eta, W, b, grad_w, grad_b):\n",
    "    \"\"\"\n",
    "        Update the parameters with an update rule\n",
    "        :param eta: the step-size\n",
    "        :param W: the weights\n",
    "        :param b: the bias\n",
    "        :param grad_w: the gradient w.r.t. the weights\n",
    "        :param grad_b: the gradient w.r.t. the bias\n",
    "        :type eta: float\n",
    "        :type W: ndarray\n",
    "        :type b: ndarray\n",
    "        :type grad_w: ndarray\n",
    "        :type grad_b: ndarray\n",
    "        :return: the updated parameters\n",
    "        :rtype: ndarray, ndarray\n",
    "    \"\"\"\n",
    "    b = b.transpose()\n",
    "    b = b - eta*grad_b\n",
    "    W = W - eta*grad_w\n",
    "    #for i in range(len(b)):\n",
    "    #    for j in range(W.shape[1]):\n",
    "    #        W[i][j]=W[i][j]-eta*grad_w[i][j] \n",
    "    return W, b\n",
    "eta=0.1\n",
    "W,b=update(eta, W, b, grad_w, grad_b)\n",
    "print(W.shape, b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8: Implémenter la fonction de calcul du coût et de la précision:\n",
    "Utiliser les fonction *forward* et *softmax*, puis calculer le coût $c$, qui est moins la log-probabilité des classes à prédire: $$c = - \\sum_{(x_{i}, y_{i}) \\in \\mathcal{D}} \\log P(y=y_{i}|x_{i},W,b)$$\n",
    "\n",
    "Et enfin la précision, c'est à dire le taux de prédictions réussies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeLoss(W, b, batch, labels, softmax):\n",
    "    \"\"\"\n",
    "        Compute the loss value of the current network on the full batch\n",
    "        :param W: the weights\n",
    "        :param B: the bias\n",
    "        :param batch: the weights\n",
    "        :param labels: the bias\n",
    "        :param act_func: the weights\n",
    "        :type W: ndarray\n",
    "        :type B: ndarray\n",
    "        :type batch: ndarray\n",
    "        :type act_func: function\n",
    "        :return loss: the negative log-likelihood\n",
    "        :return accuracy: the ratio of examples that are well-classified\n",
    "        :rtype: float\n",
    "        :rtype: float\n",
    "    \"\"\" \n",
    "    ### Forward propagation\n",
    "    ### Compute the softmax\n",
    "    loss = 0\n",
    "    accuracy = 0         \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9: Implémenter la fonction de création de mini-batch, qui pour un indice, une taille de batch, des données d'entraînement et les labels correspondants, renvoie le batch de données/labels correspondant, et la taille du batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMiniBatch(i, batch_size, train_set, one_hot):\n",
    "    \"\"\"\n",
    "        Return a minibatch from the training set and the associated labels\n",
    "        :param i: the identifier of the minibatch\n",
    "        :param batch_size: the number of training examples\n",
    "        :param train_set: the training set\n",
    "        :param one_hot: the one-hot representation of the labels\n",
    "        :type i: int\n",
    "        :type batch_size: int\n",
    "        :type train_set: ndarray\n",
    "        :type ont_hot: ndarray\n",
    "        :return: the minibatch of examples\n",
    "        :return: the minibatch of labels\n",
    "        :return: the number of examples in the minibatch\n",
    "        :rtype: ndarray\n",
    "        :rtype: ndarray\n",
    "        :rtype: int\n",
    "    \"\"\"\n",
    "    n_training = train_set[0].shape[0]\n",
    "    idx_begin = 0\n",
    "    idx_end = 0\n",
    "    mini_batch_size = 0\n",
    "\n",
    "    batch = 0\n",
    "    one_hot_batch = 0\n",
    "\n",
    "    return np.asfortranarray(batch), one_hot_batch, mini_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math,time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Data structures for plotting\n",
    "g_i = []\n",
    "g_train_loss=[]\n",
    "g_train_acc=[]\n",
    "g_valid_loss=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "n_training, n_feature, n_label = getDimDataset(train_set)\n",
    "\n",
    "# SGD parameters\n",
    "eta = 0.001\n",
    "batch_size = 500\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "n_epoch = 100\n",
    "\n",
    "cumul_time = 0.\n",
    "\n",
    "# Initialize the model parameters\n",
    "W,b = init(n_feature,n_label)\n",
    "printInfo(W,b)\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        minibatch, one_hot_batch, minibatch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Z = forward(W,b,minibatch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Z)\n",
    "\n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = gradient_out(out,one_hot_batch)\n",
    "\n",
    "        ### Compute the gradient w.r.t. parameters\n",
    "        grad_w,grad_b = gradient(derror, minibatch)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W,b = update(eta, W, b, grad_w, grad_b)\n",
    "        \n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "    \n",
    "    ### Training accuracy\n",
    "    train_loss, train_acc = computeLoss(W, b, train_set[0], train_set[1],softmax) \n",
    "    \n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_acc = computeLoss(W, b, valid_set[0], valid_set[1],softmax) \n",
    "\n",
    "    g_i = np.append(g_i, i)\n",
    "    g_train_loss = np.append(g_train_loss, train_loss)\n",
    "    g_train_acc = np.append(g_train_acc, train_acc)\n",
    "    g_valid_loss = np.append(g_valid_loss, valid_loss)\n",
    "    g_valid_acc = np.append(g_valid_acc, valid_acc)\n",
    "    \n",
    "    result_line = str(i) + \" \" + str(cumul_time) + \" \" + str(train_loss) + \" \" + str(train_acc) + \" \" + str(valid_loss) + \" \" + str(valid_acc) + \" \" + str(eta)\n",
    "    print(result_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(g_i,g_train_loss,label='train_loss')\n",
    "plt.plot(g_i,g_valid_loss,label='valid_loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Negative log-likelihood\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(g_i,1.0-g_train_acc,label='train_acc')\n",
    "plt.plot(g_i,1.0-g_valid_acc,label='valid_acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Classification error\")\n",
    "plt.ylim([0.,1.])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10: Montrer, à l'aide d'une figure, l'effet du step-size (prendre $\\eta$=[0.01,0.1,1.0,10.]) sur les courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
