{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce TP1 est d'acquérir les bases nécessaires à la compréhension des réseaux de neurones à partir d'un modèle simple de type Softmax. La tâche d'apprentissage consiste à classifier les images (28 par 28 pixels) de la base MNIST (http://yann.lecun.com/exdb/mnist/) en 10 catégories représentant les chiffres 0-9.\n",
    "\n",
    "Le TP2 consistera à généraliser les concepts de ce TP1 à un réseau de neurones multi-couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement de la base d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de la base en mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_loader\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez visualiser les différents caractères en changeant l'identifiant de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3b6043b198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_id = 900\n",
    "X=train_set[0][img_id]\n",
    "plt.imshow(X.reshape(28,28),cmap='Greys')\n",
    "print(\"label: \" + str(train_set[1][img_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Donner les caractéristiques de la base d'apprentissage train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 784 10\n"
     ]
    }
   ],
   "source": [
    "def getDimDataset(train_set):\n",
    "    n_training = len(train_set[0])\n",
    "    n_feature = len(train_set[0][0])\n",
    "    n_label = len(set(train_set[1]))\n",
    "    return n_training, n_feature, n_label\n",
    "\n",
    "(n_training,n_feature,n_label)=getDimDataset(train_set)\n",
    "print (n_training,n_feature,n_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init(n_feature,n_label):\n",
    "    sigma = 1.\n",
    "    W = np.random.normal(loc=0.0, scale=sigma/np.sqrt(n_feature), size=(n_label,n_feature))\n",
    "    b = np.zeros((W.shape[0],1))\n",
    "    return W,b\n",
    "\n",
    "W,b=init(n_feature,n_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Donner les dimensions de W et b ainsi que le nombre total de paramètres du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W dimensions: (10, 784)\n",
      "b dimensions: (10, 1)\n",
      "Number of parameters: 7850\n"
     ]
    }
   ],
   "source": [
    "def printInfo(W,b):\n",
    "    print(\"W dimensions: \" + str(W.shape))\n",
    "    print(\"b dimensions: \" + str(b.shape))\n",
    "    print(\"Number of parameters: \" + str(W.shape[0]*W.shape[1]+b.shape[0]))\n",
    "    \n",
    "printInfo(W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Implémenter la fonction forward $$z_j = \\sum_{i \\rightarrow j} W_{ij} x_i + b_j$$ où $x_i$ est un pixel de l'image, $W_{ij}$ est la valeur associée à l'arête reliant les unités $i$ et $j$ et $b_j$ est le bias associé à l'unité $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : X (784,) W (10, 784) b (1, 10)\n"
     ]
    }
   ],
   "source": [
    "def forward(W,b,X):\n",
    "    \"\"\"\n",
    "        Perform the forward propagation\n",
    "        :param W: the weights\n",
    "        :param b: the bias\n",
    "        :param X: the input (minibatch_size x n_input)\n",
    "        :type W: ndarray\n",
    "        :type B: ndarray\n",
    "        :type X: ndarray\n",
    "        :return: the transformed values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    print('forward : X', X.shape, 'W', W.shape, 'b', b.shape)\n",
    "    z = X.dot(W.transpose()) + b.transposen()\n",
    "    return z\n",
    "\n",
    "#X = train_set[0][:5] # 5 examples at once\n",
    "#print(X.shape)\n",
    "z = forward(W,b.transpose(),X) # Works with any number of examples !\n",
    "#print(z.shape)\n",
    "#print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Implémenter la fonction softmax $$ \\sigma_i = P(t=i|x,W,b) = \\frac{\\exp{z_i}}{\\sum_k \\exp{z_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "[[ 0.07405116  0.08878171  0.1582388   0.08165582  0.09199735  0.1072995\n",
      "   0.10180631  0.08481215  0.11479197  0.09656523]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "        Perform the softmax transformation to the pre-activation values\n",
    "        :param z: the pre-activation values\n",
    "        :type z: ndarray\n",
    "        :return: the activation values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    exps = np.exp(z)\n",
    "    somme_exps = np.sum(exps,axis=1)\n",
    "    for i in range(somme_exps.shape[0]):\n",
    "        exps[i]/=somme_exps[i]\n",
    "    return exps\n",
    "out=softmax(z)\n",
    "print(out.shape)\n",
    "print (out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionnel: Vérifier que votre implémentation de softmax soit numériquement stable (cf. http://ufldl.stanford.edu/wiki/index.php/Exercise:Softmax_Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: Implémenter le calcul du gradient de l'erreur par rapport à $z_i$:\n",
    "$$\\delta z_i = \\sigma_i - 1_{i=l}$$\n",
    "où $l$ est l'étiquette associée à la donnée courante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_out : out (1, 10) one_hot_batch (10,)\n",
      "(1, 10)\n",
      "[[ 0.07405116  0.08878171  0.1582388   0.08165582  0.09199735  0.1072995\n",
      "   0.10180631  0.08481215 -0.88520803  0.09656523]]\n"
     ]
    }
   ],
   "source": [
    "def gradient_out(out, one_hot_batch):\n",
    "    \"\"\"\n",
    "    compute the gradient w.r.t. the pre-activation values of the softmax z_i\n",
    "    :param out: the softmax values\n",
    "    :type out: ndarray\n",
    "    :param one_hot_batch: the one-hot representation of the labels\n",
    "    :type one_hot_batch: ndarray\n",
    "    :return: the gradient w.r.t. z\n",
    "    :rtype: ndarray\n",
    "    \"\"\"\n",
    "    print('gradient_out : out', out.shape, 'one_hot_batch', one_hot_batch.shape)\n",
    "    return out - one_hot_batch\n",
    "\n",
    "derror=gradient_out(out,np.array([0,0,0,0,0,0,0,0,1,0]))\n",
    "print(derror.shape)\n",
    "print(derror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: Implémenter la fonction du calcul de gradient par rapport aux paramètres: $$\\delta W_{ij} = \\delta z_j x_i$$  $$\\delta b_{j} = \\delta z_j$$ où $\\delta W_{ij}$ est la composante du gradient associée à l'arête reliant les unités $i$ et $j$, $\\delta b_{j}$ est la composante du gradient associée au bias de l'unité $j$, $\\delta z_j$ est le gradient de l'erreur par rapport à l'unité $j$ et $x_i$ est la valeur d'activation de l'unité $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10) (784,)\n",
      "gradient_update: grad_w (10, 784) grad_b (1, 10)\n",
      "(10, 784) (1, 10)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.07405116  0.08878171  0.1582388   0.08165582  0.09199735  0.1072995\n",
      "   0.10180631  0.08481215 -0.88520803  0.09656523]]\n"
     ]
    }
   ],
   "source": [
    "def gradient(derror, X):\n",
    "    \"\"\"\n",
    "        Compute the gradient w.r.t. the parameters\n",
    "        :param derror: the gradient w.r.t. z\n",
    "        :param X: the input (minibatch_size x n_input)\n",
    "        :param minibatch_size: the minibatch size\n",
    "        :type derror: ndarray\n",
    "        :type minibatch: ndarray\n",
    "        :type minibatch_size: unsigned\n",
    "        :return: the gradient w.r.t. the parameters\n",
    "        :rtype: ndarray, ndarray\n",
    "    \"\"\"     \n",
    "    #grad_w = np.zeros((derror.shape[0],X.shape[0]))\n",
    "    #grad_b = np.zeros((derror.shape[0]))\n",
    "    #print(grad_w.shape, grad_b.shape)\n",
    "    #for j in range(derror.shape[0]):\n",
    "    #    grad_b[j]=derror[j]\n",
    "    #    for i in range(X.shape[0]):\n",
    "    #        grad_w[j][i]=derror[j]*X[i]\n",
    "    grad_b = derror\n",
    "    grad_w = derror.transpose().dot(X.reshape(-1, 784))\n",
    "    print('gradient_update: grad_w', grad_w.shape, 'grad_b', grad_b.shape)\n",
    "    return grad_w,grad_b\n",
    "\n",
    "print(derror.shape, X.shape)\n",
    "grad_w, grad_b=gradient(derror,X)\n",
    "print(grad_w.shape, grad_b.shape)\n",
    "print(grad_w)\n",
    "print(grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7: Implémenter la fonction de mise à jour des paramètres $$p = p - \\eta \\delta p$$ où $p$ est un paramètre du modèle et $\\delta p$ la composante du gradient associée à p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update : W (10, 784) b (1, 10) grad_w (10, 784) grad_b (1, 10)\n",
      "(10, 784) (1, 10)\n",
      "[[-0.00740512 -0.00887817 -0.01582388 -0.00816558 -0.00919974 -0.01072995\n",
      "  -0.01018063 -0.00848122  0.0885208  -0.00965652]]\n"
     ]
    }
   ],
   "source": [
    "def update(eta, W, b, grad_w, grad_b):\n",
    "    \"\"\"\n",
    "        Update the parameters with an update rule\n",
    "        :param eta: the step-size\n",
    "        :param W: the weights\n",
    "        :param b: the bias\n",
    "        :param grad_w: the gradient w.r.t. the weights\n",
    "        :param grad_b: the gradient w.r.t. the bias\n",
    "        :type eta: float\n",
    "        :type W: ndarray\n",
    "        :type b: ndarray\n",
    "        :type grad_w: ndarray\n",
    "        :type grad_b: ndarray\n",
    "        :return: the updated parameters\n",
    "        :rtype: ndarray, ndarray\n",
    "    \"\"\"\n",
    "    b = b.transpose()\n",
    "    b = b - eta*grad_b\n",
    "    W = W - eta*grad_w\n",
    "    print('update : W', W.shape, 'b', b.shape, 'grad_w', grad_w.shape, 'grad_b', grad_b.shape)\n",
    "    return W, b\n",
    "eta=0.1\n",
    "W,b=update(eta, W, b, grad_w, grad_b)\n",
    "print(W.shape, b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8: Implémenter la fonction de calcul du coût et de la précision:\n",
    "Utiliser les fonction *forward* et *softmax*, puis calculer le coût $c$, qui est moins la log-probabilité des classes à prédire: $$c = - \\sum_{(x_{i}, y_{i}) \\in \\mathcal{D}} \\log P(y=y_{i}|x_{i},W,b)$$\n",
    "\n",
    "Et enfin la précision, c'est à dire le taux de prédictions réussies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeLoss(W, b, batch, labels, softmax):\n",
    "    \"\"\"\n",
    "        Compute the loss value of the current network on the full batch\n",
    "        :param W: the weights\n",
    "        :param B: the bias\n",
    "        :param batch: the weights\n",
    "        :param labels: the bias\n",
    "        :param act_func: the weights\n",
    "        :type W: ndarray\n",
    "        :type B: ndarray\n",
    "        :type batch: ndarray\n",
    "        :type act_func: function\n",
    "        :return loss: the negative log-likelihood\n",
    "        :return accuracy: the ratio of examples that are well-classified\n",
    "        :rtype: float\n",
    "        :rtype: float\n",
    "    \"\"\" \n",
    "    ### Forward propagation\n",
    "    z = forward(W,b,batch)\n",
    "    ### Compute the softmax\n",
    "    out=softmax(z)   \n",
    "    out =out[labels==0]=1\n",
    "    loss = -np.sum(np.log(out))\n",
    "    accuracy = 0  \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9: Implémenter la fonction de création de mini-batch, qui pour un indice, une taille de batch, des données d'entraînement et les labels correspondants, renvoie le batch de données/labels correspondant, et la taille du batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMiniBatch(i, batch_size, train_set, one_hot):\n",
    "    \"\"\"\n",
    "        Return a minibatch from the training set and the associated labels\n",
    "        :param i: the identifier of the minibatch\n",
    "        :param batch_size: the number of training examples\n",
    "        :param train_set: the training set\n",
    "        :param one_hot: the one-hot representation of the labels\n",
    "        :type i: int\n",
    "        :type batch_size: int\n",
    "        :type train_set: ndarray\n",
    "        :type ont_hot: ndarray\n",
    "        :return: the minibatch of examples\n",
    "        :return: the minibatch of labels\n",
    "        :return: the number of examples in the minibatch\n",
    "        :rtype: ndarray\n",
    "        :rtype: ndarray\n",
    "        :rtype: int\n",
    "    \"\"\"\n",
    "    n_training = train_set[0].shape[0]\n",
    "    idx_begin = i\n",
    "    idx_end = i+batch_size\n",
    "    batch = train_set[0][idx_begin:idx_end]\n",
    "    one_hot=one_hot.transpose()\n",
    "    one_hot_batch = one_hot[idx_begin:idx_end,:]\n",
    "    mini_batch_size = batch.shape[0]\n",
    "\n",
    "    return np.asfortranarray(batch), one_hot_batch, mini_batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W dimensions: (10, 784)\n",
      "b dimensions: (10, 1)\n",
      "Number of parameters: 7850\n",
      "one-hot (10, 50000)\n",
      "W (10, 784)\n",
      "b (10, 1)\n",
      "mini batch (500, 784)\n",
      "one-hot batch (500, 10)\n",
      "forward : X (500, 784) W (10, 784) b (1, 10)\n",
      "gradient_out : out (500, 10) one_hot_batch (500, 10)\n",
      "gradient_update: grad_w (10, 784) grad_b (500, 10)\n",
      "update : W (10, 784) b (500, 10) grad_w (10, 784) grad_b (500, 10)\n",
      "mini batch (500, 784)\n",
      "one-hot batch (500, 10)\n",
      "forward : X (500, 784) W (10, 784) b (10, 500)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (500,10) (10,500) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-9538973d09d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m### Forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m### Compute the softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-6e6eec7b5cd0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(W, b, X)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'forward : X'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'W'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (500,10) (10,500) "
     ]
    }
   ],
   "source": [
    "import math,time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Data structures for plotting\n",
    "g_i = []\n",
    "g_train_loss=[]\n",
    "g_train_acc=[]\n",
    "g_valid_loss=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "n_training, n_feature, n_label = getDimDataset(train_set)\n",
    "\n",
    "# SGD parameters\n",
    "eta = 0.001\n",
    "batch_size = 500\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "n_epoch = 100\n",
    "\n",
    "cumul_time = 0.\n",
    "\n",
    "# Initialize the model parameters\n",
    "W,b = init(n_feature,n_label)\n",
    "printInfo(W,b)\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "print('one-hot', one_hot.shape)\n",
    "print('W', W.shape)\n",
    "print('b', b.shape)\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        minibatch, one_hot_batch, minibatch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "        print('mini batch', minibatch.shape)\n",
    "        print('one-hot batch', one_hot_batch.shape)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Z = forward(W,b,minibatch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Z)\n",
    "\n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = gradient_out(out,one_hot_batch)\n",
    "\n",
    "        ### Compute the gradient w.r.t. parameters\n",
    "        grad_w,grad_b = gradient(derror, minibatch)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W,b = update(eta, W, b, grad_w, grad_b)\n",
    "        \n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "    \n",
    "    ### Training accuracy\n",
    "    train_loss, train_acc = computeLoss(W, b, train_set[0], train_set[1],softmax) \n",
    "    \n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_acc = computeLoss(W, b, valid_set[0], valid_set[1],softmax) \n",
    "\n",
    "    g_i = np.append(g_i, i)\n",
    "    g_train_loss = np.append(g_train_loss, train_loss)\n",
    "    g_train_acc = np.append(g_train_acc, train_acc)\n",
    "    g_valid_loss = np.append(g_valid_loss, valid_loss)\n",
    "    g_valid_acc = np.append(g_valid_acc, valid_acc)\n",
    "    \n",
    "    result_line = str(i) + \" \" + str(cumul_time) + \" \" + str(train_loss) + \" \" + str(train_acc) + \" \" + str(valid_loss) + \" \" + str(valid_acc) + \" \" + str(eta)\n",
    "    print(result_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(g_i,g_train_loss,label='train_loss')\n",
    "plt.plot(g_i,g_valid_loss,label='valid_loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Negative log-likelihood\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(g_i,1.0-g_train_acc,label='train_acc')\n",
    "plt.plot(g_i,1.0-g_valid_acc,label='valid_acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Classification error\")\n",
    "plt.ylim([0.,1.])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10: Montrer, à l'aide d'une figure, l'effet du step-size (prendre $\\eta$=[0.01,0.1,1.0,10.]) sur les courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
