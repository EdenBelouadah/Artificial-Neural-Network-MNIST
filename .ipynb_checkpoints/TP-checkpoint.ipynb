{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce TP1 est d'acquérir les bases nécessaires à la compréhension des réseaux de neurones à partir d'un modèle simple de type Softmax. La tâche d'apprentissage consiste à classifier les images (28 par 28 pixels) de la base MNIST (http://yann.lecun.com/exdb/mnist/) en 10 catégories représentant les chiffres 0-9.\n",
    "\n",
    "Le TP2 consistera à généraliser les concepts de ce TP1 à un réseau de neurones multi-couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement de la base d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de la base en mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_loader\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez visualiser les différents caractères en changeant l'identifiant de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFOEHkiL8QFKEH0iK8ANJEX4gqf8Hm8Ym39KHKIkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8b6f710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_id = 900\n",
    "X=train_set[0][img_id]\n",
    "plt.imshow(X.reshape(28,28),cmap='Greys')\n",
    "print(\"label: \" + str(train_set[1][img_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Donner les caractéristiques de la base d'apprentissage train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784, 10)\n"
     ]
    }
   ],
   "source": [
    "def getDimDataset(train_set):\n",
    "    n_training = len(train_set[0])\n",
    "    n_feature = len(train_set[0][0])\n",
    "    n_label = len(set(train_set[1]))\n",
    "    return n_training, n_feature, n_label\n",
    "\n",
    "(n_training,n_feature,n_label)=getDimDataset(train_set)\n",
    "print (n_training,n_feature,n_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init(n_feature,n_label):\n",
    "    sigma = 1.\n",
    "    W = np.random.normal(loc=0.0, scale=sigma/np.sqrt(n_feature), size=(n_label,n_feature))\n",
    "    b = np.zeros((W.shape[0],1))\n",
    "    return W,b\n",
    "\n",
    "W,b=init(n_feature,n_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Donner les dimensions de W et b ainsi que le nombre total de paramètres du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W dimensions: (10L, 784L)\n",
      "b dimensions: (10L, 1L)\n",
      "Number of parameters: 7850\n"
     ]
    }
   ],
   "source": [
    "def printInfo(W,b):\n",
    "    print(\"W dimensions: \" + str(W.shape))\n",
    "    print(\"b dimensions: \" + str(b.shape))\n",
    "    print(\"Number of parameters: \" + str(W.shape[0]*W.shape[1]+b.shape[0]))\n",
    "    \n",
    "printInfo(W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Implémenter la fonction forward $$z_j = \\sum_{i \\rightarrow j} W_{ij} x_i + b_j$$ où $x_i$ est un pixel de l'image, $W_{ij}$ est la valeur associée à l'arête reliant les unités $i$ et $j$ et $b_j$ est le bias associé à l'unité $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward(W,b,X):\n",
    "    \"\"\"\n",
    "        Perform the forward propagation\n",
    "        :param W: the weights\n",
    "        :param b: the bias\n",
    "        :param X: the input (minibatch_size x n_input)\n",
    "        :type W: ndarray\n",
    "        :type B: ndarray\n",
    "        :type X: ndarray\n",
    "        :return: the transformed values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    print 'hi'\n",
    "    print X.shape\n",
    "    print W.transpose().shape\n",
    "    print b.transpose().shape\n",
    "    print 'ho'\n",
    "    z = X.dot(W.transpose()) + b.transpose()\n",
    "    \n",
    "    return z\n",
    "\n",
    "#X = train_set[0][:5] # 5 examples at once\n",
    "#print(X.shape)\n",
    "#z = forward(W,b,X) # Works with any number of examples !\n",
    "#print(z.shape)\n",
    "#print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Implémenter la fonction softmax $$ \\sigma_i = P(t=i|x,W,b) = \\frac{\\exp{z_i}}{\\sum_k \\exp{z_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5L, 10L)\n",
      "[[ 0.08204443  0.11028055  0.1376642   0.07960897  0.11751947  0.10879575\n",
      "   0.06437883  0.13250253  0.07931046  0.0878948 ]\n",
      " [ 0.09052881  0.05367797  0.07661821  0.07460423  0.15954406  0.09976095\n",
      "   0.06636501  0.17374002  0.13561441  0.06954634]\n",
      " [ 0.09205332  0.05761376  0.10180692  0.05532477  0.19750876  0.07576219\n",
      "   0.0905666   0.1172927   0.083508    0.128563  ]\n",
      " [ 0.13139392  0.04993558  0.07761503  0.05130931  0.12990598  0.09770055\n",
      "   0.09721579  0.13576598  0.11885621  0.11030165]\n",
      " [ 0.07087986  0.05910571  0.10111046  0.08821405  0.16793736  0.12393176\n",
      "   0.05933146  0.14153615  0.14159353  0.04635968]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "        Perform the softmax transformation to the pre-activation values\n",
    "        :param z: the pre-activation values\n",
    "        :type z: ndarray\n",
    "        :return: the activation values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    exps = np.exp(z)\n",
    "    somme_exps = np.sum(exps,axis=1)\n",
    "    for i in range(somme_exps.shape[0]):\n",
    "        exps[i]/=somme_exps[i]\n",
    "    return exps\n",
    "out=softmax(z)\n",
    "print(out.shape)\n",
    "print (out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionnel: Vérifier que votre implémentation de softmax soit numériquement stable (cf. http://ufldl.stanford.edu/wiki/index.php/Exercise:Softmax_Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: Implémenter le calcul du gradient de l'erreur par rapport à $z_i$:\n",
    "$$\\delta z_i = \\sigma_i - 1_{i=l}$$\n",
    "où $l$ est l'étiquette associée à la donnée courante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5L, 10L)\n",
      "[[ 0.08204443  0.11028055  0.1376642   0.07960897  0.11751947  0.10879575\n",
      "   0.06437883  0.13250253 -0.92068954  0.0878948 ]\n",
      " [ 0.09052881  0.05367797  0.07661821  0.07460423  0.15954406  0.09976095\n",
      "   0.06636501  0.17374002 -0.86438559  0.06954634]\n",
      " [ 0.09205332  0.05761376  0.10180692  0.05532477  0.19750876  0.07576219\n",
      "   0.0905666   0.1172927  -0.916492    0.128563  ]\n",
      " [ 0.13139392  0.04993558  0.07761503  0.05130931  0.12990598  0.09770055\n",
      "   0.09721579  0.13576598 -0.88114379  0.11030165]\n",
      " [ 0.07087986  0.05910571  0.10111046  0.08821405  0.16793736  0.12393176\n",
      "   0.05933146  0.14153615 -0.85840647  0.04635968]]\n"
     ]
    }
   ],
   "source": [
    "def gradient_out(out, one_hot_batch):\n",
    "    \"\"\"\n",
    "    compute the gradient w.r.t. the pre-activation values of the softmax z_i\n",
    "    :param out: the softmax values\n",
    "    :type out: ndarray\n",
    "    :param one_hot_batch: the one-hot representation of the labels\n",
    "    :type one_hot_batch: ndarray\n",
    "    :return: the gradient w.r.t. z\n",
    "    :rtype: ndarray\n",
    "    \"\"\"   \n",
    "    return out - one_hot_batch\n",
    "\n",
    "derror=gradient_out(out,np.array([0,0,0,0,0,0,0,0,1,0]))\n",
    "print(derror.shape)\n",
    "print(derror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: Implémenter la fonction du calcul de gradient par rapport aux paramètres: $$\\delta W_{ij} = \\delta z_j x_i$$  $$\\delta b_{j} = \\delta z_j$$ où $\\delta W_{ij}$ est la composante du gradient associée à l'arête reliant les unités $i$ et $j$, $\\delta b_{j}$ est la composante du gradient associée au bias de l'unité $j$, $\\delta z_j$ est le gradient de l'erreur par rapport à l'unité $j$ et $x_i$ est la valeur d'activation de l'unité $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((5L, 10L), (5L, 784L))\n",
      "((10L, 784L), (5L, 10L))\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.08204443  0.11028055  0.1376642   0.07960897  0.11751947  0.10879575\n",
      "   0.06437883  0.13250253 -0.92068954  0.0878948 ]\n",
      " [ 0.09052881  0.05367797  0.07661821  0.07460423  0.15954406  0.09976095\n",
      "   0.06636501  0.17374002 -0.86438559  0.06954634]\n",
      " [ 0.09205332  0.05761376  0.10180692  0.05532477  0.19750876  0.07576219\n",
      "   0.0905666   0.1172927  -0.916492    0.128563  ]\n",
      " [ 0.13139392  0.04993558  0.07761503  0.05130931  0.12990598  0.09770055\n",
      "   0.09721579  0.13576598 -0.88114379  0.11030165]\n",
      " [ 0.07087986  0.05910571  0.10111046  0.08821405  0.16793736  0.12393176\n",
      "   0.05933146  0.14153615 -0.85840647  0.04635968]]\n"
     ]
    }
   ],
   "source": [
    "def gradient(derror, X):\n",
    "    \"\"\"\n",
    "        Compute the gradient w.r.t. the parameters\n",
    "        :param derror: the gradient w.r.t. z\n",
    "        :param X: the input (minibatch_size x n_input)\n",
    "        :param minibatch_size: the minibatch size\n",
    "        :type derror: ndarray\n",
    "        :type minibatch: ndarray\n",
    "        :type minibatch_size: unsigned\n",
    "        :return: the gradient w.r.t. the parameters\n",
    "        :rtype: ndarray, ndarray\n",
    "    \"\"\"     \n",
    "    #grad_w = np.zeros((derror.shape[0],X.shape[0]))\n",
    "    #grad_b = np.zeros((derror.shape[0]))\n",
    "    #print(grad_w.shape, grad_b.shape)\n",
    "    #for j in range(derror.shape[0]):\n",
    "    #    grad_b[j]=derror[j]\n",
    "    #    for i in range(X.shape[0]):\n",
    "    #        grad_w[j][i]=derror[j]*X[i]\n",
    "    grad_b = derror\n",
    "    grad_w = derror.transpose().dot(X.reshape(-1, 784))\n",
    "    return grad_w,grad_b\n",
    "\n",
    "print(derror.shape, X.shape)\n",
    "grad_w, grad_b=gradient(derror,X)\n",
    "print(grad_w.shape, grad_b.shape)\n",
    "print(grad_w)\n",
    "print(grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7: Implémenter la fonction de mise à jour des paramètres $$p = p - \\eta \\delta p$$ où $p$ est un paramètre du modèle et $\\delta p$ la composante du gradient associée à p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10L, 784L), (5L, 10L))\n",
      "[[-0.00820444 -0.01102806 -0.01376642 -0.0079609  -0.01175195 -0.01087958\n",
      "  -0.00643788 -0.01325025  0.09206895 -0.00878948]\n",
      " [-0.00905288 -0.0053678  -0.00766182 -0.00746042 -0.01595441 -0.00997609\n",
      "  -0.0066365  -0.017374    0.08643856 -0.00695463]\n",
      " [-0.00920533 -0.00576138 -0.01018069 -0.00553248 -0.01975088 -0.00757622\n",
      "  -0.00905666 -0.01172927  0.0916492  -0.0128563 ]\n",
      " [-0.01313939 -0.00499356 -0.0077615  -0.00513093 -0.0129906  -0.00977006\n",
      "  -0.00972158 -0.0135766   0.08811438 -0.01103017]\n",
      " [-0.00708799 -0.00591057 -0.01011105 -0.00882141 -0.01679374 -0.01239318\n",
      "  -0.00593315 -0.01415362  0.08584065 -0.00463597]]\n"
     ]
    }
   ],
   "source": [
    "def update(eta, W, b, grad_w, grad_b):\n",
    "    \"\"\"\n",
    "        Update the parameters with an update rule\n",
    "        :param eta: the step-size\n",
    "        :param W: the weights\n",
    "        :param b: the bias\n",
    "        :param grad_w: the gradient w.r.t. the weights\n",
    "        :param grad_b: the gradient w.r.t. the bias\n",
    "        :type eta: float\n",
    "        :type W: ndarray\n",
    "        :type b: ndarray\n",
    "        :type grad_w: ndarray\n",
    "        :type grad_b: ndarray\n",
    "        :return: the updated parameters\n",
    "        :rtype: ndarray, ndarray\n",
    "    \"\"\"\n",
    "    b = b.transpose()\n",
    "    b = b - eta*grad_b\n",
    "    W = W - eta*grad_w\n",
    "    return W, b\n",
    "eta=0.1\n",
    "W,b=update(eta, W, b, grad_w, grad_b)\n",
    "print(W.shape, b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8: Implémenter la fonction de calcul du coût et de la précision:\n",
    "Utiliser les fonction *forward* et *softmax*, puis calculer le coût $c$, qui est moins la log-probabilité des classes à prédire: $$c = - \\sum_{(x_{i}, y_{i}) \\in \\mathcal{D}} \\log P(y=y_{i}|x_{i},W,b)$$\n",
    "\n",
    "Et enfin la précision, c'est à dire le taux de prédictions réussies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLoss(W, b, batch, labels, softmax):\n",
    "    \"\"\"\n",
    "        Compute the loss value of the current network on the full batch\n",
    "        :param W: the weights\n",
    "        :param B: the bias\n",
    "        :param batch: the weights\n",
    "        :param labels: the bias\n",
    "        :param act_func: the weights\n",
    "        :type W: ndarray\n",
    "        :type B: ndarray\n",
    "        :type batch: ndarray\n",
    "        :type act_func: function\n",
    "        :return loss: the negative log-likelihood\n",
    "        :return accuracy: the ratio of examples that are well-classified\n",
    "        :rtype: float\n",
    "        :rtype: float\n",
    "    \"\"\" \n",
    "    ### Forward propagation\n",
    "    z = forward(W,b,batch)\n",
    "    ### Compute the softmax\n",
    "    out=softmax(z)   \n",
    "    out =out[labels==0]=1\n",
    "    loss = -np.sum(np.log(out))\n",
    "    accuracy = 0  \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9: Implémenter la fonction de création de mini-batch, qui pour un indice, une taille de batch, des données d'entraînement et les labels correspondants, renvoie le batch de données/labels correspondant, et la taille du batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMiniBatch(i, batch_size, train_set, one_hot):\n",
    "    \"\"\"\n",
    "        Return a minibatch from the training set and the associated labels\n",
    "        :param i: the identifier of the minibatch\n",
    "        :param batch_size: the number of training examples\n",
    "        :param train_set: the training set\n",
    "        :param one_hot: the one-hot representation of the labels\n",
    "        :type i: int\n",
    "        :type batch_size: int\n",
    "        :type train_set: ndarray\n",
    "        :type ont_hot: ndarray\n",
    "        :return: the minibatch of examples\n",
    "        :return: the minibatch of labels\n",
    "        :return: the number of examples in the minibatch\n",
    "        :rtype: ndarray\n",
    "        :rtype: ndarray\n",
    "        :rtype: int\n",
    "    \"\"\"\n",
    "    n_training = train_set[0].shape[0]\n",
    "    idx_begin = i\n",
    "    idx_end = i+batch_size\n",
    "    batch = train_set[0][idx_begin:idx_end]\n",
    "    one_hot=one_hot.transpose()\n",
    "    one_hot_batch = one_hot[idx_begin:idx_end,:]\n",
    "    mini_batch_size = batch.shape[0]\n",
    "    print one_hot_batch.shape\n",
    "\n",
    "    return np.asfortranarray(batch), one_hot_batch, mini_batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W dimensions: (10L, 784L)\n",
      "b dimensions: (10L, 1L)\n",
      "Number of parameters: 7850\n",
      "(10L, 50000L)\n",
      "(500L, 10L)\n",
      "hi\n",
      "(500L, 784L)\n",
      "(784L, 10L)\n",
      "(1L, 10L)\n",
      "ho\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (500,10) (10,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-aa40f0a2cca4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m### Forward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m### Compute the softmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-f20235afcdd2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(W, b, X)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'ho'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (500,10) (10,1) "
     ]
    }
   ],
   "source": [
    "import math,time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Data structures for plotting\n",
    "g_i = []\n",
    "g_train_loss=[]\n",
    "g_train_acc=[]\n",
    "g_valid_loss=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "n_training, n_feature, n_label = getDimDataset(train_set)\n",
    "\n",
    "# SGD parameters\n",
    "eta = 0.001\n",
    "batch_size = 500\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "n_epoch = 100\n",
    "\n",
    "cumul_time = 0.\n",
    "\n",
    "# Initialize the model parameters\n",
    "W,b = init(n_feature,n_label)\n",
    "printInfo(W,b)\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "print one_hot.shape\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        minibatch, one_hot_batch, minibatch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Z = forward(W,b,minibatch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Z)\n",
    "\n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = gradient_out(out,one_hot_batch)\n",
    "\n",
    "        ### Compute the gradient w.r.t. parameters\n",
    "        grad_w,grad_b = gradient(derror, minibatch)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W,b = update(eta, W, b, grad_w, grad_b)\n",
    "        \n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "    \n",
    "    ### Training accuracy\n",
    "    train_loss, train_acc = computeLoss(W, b, train_set[0], train_set[1],softmax) \n",
    "    \n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_acc = computeLoss(W, b, valid_set[0], valid_set[1],softmax) \n",
    "\n",
    "    g_i = np.append(g_i, i)\n",
    "    g_train_loss = np.append(g_train_loss, train_loss)\n",
    "    g_train_acc = np.append(g_train_acc, train_acc)\n",
    "    g_valid_loss = np.append(g_valid_loss, valid_loss)\n",
    "    g_valid_acc = np.append(g_valid_acc, valid_acc)\n",
    "    \n",
    "    result_line = str(i) + \" \" + str(cumul_time) + \" \" + str(train_loss) + \" \" + str(train_acc) + \" \" + str(valid_loss) + \" \" + str(valid_acc) + \" \" + str(eta)\n",
    "    print(result_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(g_i,g_train_loss,label='train_loss')\n",
    "plt.plot(g_i,g_valid_loss,label='valid_loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Negative log-likelihood\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(g_i,1.0-g_train_acc,label='train_acc')\n",
    "plt.plot(g_i,1.0-g_valid_acc,label='valid_acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Classification error\")\n",
    "plt.ylim([0.,1.])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10: Montrer, à l'aide d'une figure, l'effet du step-size (prendre $\\eta$=[0.01,0.1,1.0,10.]) sur les courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
